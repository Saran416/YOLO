{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import  xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import shutil\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images=\"/home/saran/Desktop/Python Projects/Object Detection/PNGImages/\"\n",
    "train_maps=\"/home/saran/Desktop/Python Projects/Object Detection/Annotations/\"\n",
    "\n",
    "classes=['bicycle','car','motorbike','person']\n",
    "\n",
    "class_dict = {classes[i]: i for i in range(len(classes))}\n",
    "\n",
    "N_CLASSES=4\n",
    "H,W=224,224\n",
    "SPLIT_SIZE=7\n",
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bicycle': 0, 'car': 1, 'motorbike': 2, 'person': 3}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function preprocesses a .txt file containing object detection annotations.\n",
    "def preprocess_txt(filename, class_name):\n",
    "    height = 0\n",
    "    width = 0\n",
    "    xmin = 0\n",
    "    ymin = 0\n",
    "    xmax = 0\n",
    "    ymax = 0\n",
    "    \n",
    "    # Open and read the .txt file.\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if(line.find(\"Image size\") != -1):\n",
    "                width = float(line.split(\":\")[1].split(\"x\")[0])\n",
    "                height = float(line.split(\":\")[1].split(\"x\")[1])\n",
    "            elif (line.find(\"Bounding box for object\") != -1):\n",
    "                temp = line.split(':')[1]\n",
    "                numbers = re.findall(r'\\d+', temp)\n",
    "                xmin, ymin, xmax, ymax = [int(num) for num in numbers]\n",
    "                \n",
    "                # Calculate normalized bounding box coordinates.\n",
    "                bounding_box = [\n",
    "                    (xmin + xmax) / (2 * width),\n",
    "                    (ymin + ymax) / (2 * height),\n",
    "                    (xmax - xmin) / width,\n",
    "                    (ymax - ymin) / height,\n",
    "                    class_dict[class_name]\n",
    "                ]\n",
    "    \n",
    "    # Convert the list of bounding boxes to a TensorFlow tensor and return.\n",
    "    return bounding_box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'imshow'\n> Overload resolution failed:\n>  - imshow() missing required argument 'mat' (pos 2)\n>  - imshow() missing required argument 'mat' (pos 2)\n>  - imshow() missing required argument 'mat' (pos 2)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m cv2\u001b[38;5;241m.\u001b[39mrectangle(image, top_left, bottom_right, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Display the image in a window\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Wait for a key press and close the window\u001b[39;00m\n\u001b[1;32m     18\u001b[0m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'imshow'\n> Overload resolution failed:\n>  - imshow() missing required argument 'mat' (pos 2)\n>  - imshow() missing required argument 'mat' (pos 2)\n>  - imshow() missing required argument 'mat' (pos 2)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text to extract bounding box coordinates for the car\n",
    "bbox = preprocess_txt(\"/home/saran/Desktop/Python Projects/Object Detection/Annotations/Caltech_cars/image_0001.txt\", \"car\")\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(\"/home/saran/Desktop/Python Projects/Object Detection/PNGImages/Caltech_cars/image_0001.png\")\n",
    "\n",
    "# Calculate the top-left and bottom-right corners of the bounding box\n",
    "top_left = (int((bbox[0] - bbox[2] / 2) * 640), int((bbox[1] - bbox[3] / 2) * 480))\n",
    "bottom_right = (int((bbox[0] + bbox[2] / 2) * 640), int((bbox[1] + bbox[3] / 2) * 480))\n",
    "\n",
    "# Draw the rectangle on the image\n",
    "cv2.rectangle(image, top_left, bottom_right, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image in a window\n",
    "cv2.imshow(bbox[-1],image)\n",
    "\n",
    "# Wait for a key press and close the window\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_boxes = []\n",
    "for cn in os.listdir(train_maps):\n",
    "    for file in os.listdir(train_maps + cn):\n",
    "        file_path = train_maps + cn + \"/\" + file\n",
    "        class_name = \"\"\n",
    "        if(cn == \"Caltech_motorbikes_side\"):\n",
    "            class_name = \"motorbike\"\n",
    "        elif(cn == \"ETHZ_motorbike-testset\"):\n",
    "            class_name = \"motorbike\"\n",
    "        elif(cn == \"TUGraz_bike\"):\n",
    "            class_name = \"bicycle\"\n",
    "        elif(cn == \"TUGraz_cars\"):\n",
    "            class_name = \"car\"\n",
    "        elif(cn == \"Caltech_cars\"):\n",
    "            class_name = \"car\"\n",
    "        elif(cn == \"ETHZ_sideviews-cars\"):\n",
    "            class_name = \"car\"\n",
    "        elif(cn == \"TUGraz_person\"):\n",
    "            class_name = \"person\"\n",
    "        bb = preprocess_txt(file_path, class_name=class_name)\n",
    "        bounding_boxes.append(bb)\n",
    "\n",
    "bounding_boxes = torch.Tensor(bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(bounding_boxes):\n",
    "    # Initialize output label tensor\n",
    "    output_label = np.zeros((SPLIT_SIZE, SPLIT_SIZE, N_CLASSES + 5))\n",
    "\n",
    "    # Iterate through bounding boxes\n",
    "    for b in range(len(bounding_boxes)):\n",
    "        # Calculate grid positions\n",
    "        grid_x = bounding_boxes[..., b, 0] * SPLIT_SIZE\n",
    "        grid_y = bounding_boxes[..., b, 1] * SPLIT_SIZE\n",
    "        \n",
    "        # Convert to integer grid indices\n",
    "        i = int(grid_x)\n",
    "        j = int(grid_y)\n",
    "\n",
    "        # Assign values to output label tensor\n",
    "        output_label[i, j, 0:5] = [1., grid_x % 1, grid_y % 1, bounding_boxes[..., b, 2], bounding_boxes[..., b, 3]]\n",
    "        output_label[i, j, 5 + int(bounding_boxes[..., b, 4])] = 1.\n",
    "    return torch.tensor(output_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.0000, 0.2480, 0.2293, 0.0630, 0.1073, 0.0000, 1.0000, 0.0000, 0.0000],\n",
       "        [1.0000, 0.8094, 0.6760, 0.0844, 0.2771, 1.0000, 1.0000, 0.0000, 1.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = generate_output(bounding_boxes)\n",
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_paths=[]\n",
    "xml_paths=[]\n",
    "\n",
    "for i in os.listdir(train_maps)[:5000]:\n",
    "    im_paths.append(train_images+i[:-3]+'jpg')\n",
    "    xml_paths.append(train_maps+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((H, W)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "images = []\n",
    "for im_path in im_paths:\n",
    "    # Open image using PIL\n",
    "    img = Image.open(im_path).convert('RGB')\n",
    "    \n",
    "    # Apply transformations\n",
    "    img_tensor = transform(img)\n",
    "    \n",
    "    images.append(img_tensor)\n",
    "\n",
    "# Stack tensors\n",
    "images = torch.stack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = []\n",
    "\n",
    "for xml_path in xml_paths:\n",
    "    \n",
    "    boxes.append(generate_output(preprocess_xml(xml_path)))\n",
    "    \n",
    "boxes = torch.tensor(boxes,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "NUM_FILTERS = 512\n",
    "OUTPUT_DIM = int(N_CLASSES + 5)\n",
    "\n",
    "# Load pre-trained ResNet50\n",
    "base_model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Remove the last fully connected layer\n",
    "base_model = nn.Sequential(*list(base_model.children())[:-2])\n",
    "\n",
    "# Freeze the base model\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# Define additional layers\n",
    "conv_layers = nn.Sequential(\n",
    "    nn.Conv2d(2048, NUM_FILTERS, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(NUM_FILTERS),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    \n",
    "    nn.Conv2d(NUM_FILTERS, NUM_FILTERS, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(NUM_FILTERS),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    \n",
    "    nn.Conv2d(NUM_FILTERS, NUM_FILTERS, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(NUM_FILTERS),\n",
    "    nn.LeakyReLU(0.1)\n",
    ")\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "\n",
    "dense_layers = nn.Sequential(\n",
    "    nn.Linear(NUM_FILTERS * (H // 32) * (W // 32), NUM_FILTERS),\n",
    "    nn.BatchNorm1d(NUM_FILTERS),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    \n",
    "    nn.Linear(NUM_FILTERS, int(SPLIT_SIZE * SPLIT_SIZE * OUTPUT_DIM)),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Combine all layers into a single sequential model\n",
    "model = nn.Sequential(\n",
    "    base_model,\n",
    "    conv_layers,\n",
    "    flatten,\n",
    "    dense_layers,\n",
    "    nn.Unflatten(1, (SPLIT_SIZE, SPLIT_SIZE, OUTPUT_DIM))\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(x, y):\n",
    "    return torch.sum((y - x)**2)\n",
    "\n",
    "def yolo_loss(y_true, y_pred):\n",
    "    target = y_true[..., 0]  # Grid cell where we have information of object presence\n",
    "    \n",
    "    #--------------------------------------- for object\n",
    "    \n",
    "    target_indices = target == 1\n",
    "    y_pred_extract = y_pred[target_indices]\n",
    "    y_target_extract = y_true[target_indices]\n",
    "    \n",
    "    object_loss = difference(y_pred_extract[..., 0].float(), torch.ones_like(y_pred_extract[..., 0]).float())\n",
    "    \n",
    "    #------------------------------------------------------ for no object\n",
    "    \n",
    "    target_indices_no_obj = target == 0\n",
    "    y_pred_extract_no_obj = y_pred[target_indices_no_obj]\n",
    "    y_target_extract_no_obj = y_true[target_indices_no_obj]\n",
    "\n",
    "    no_obj_loss = difference(y_pred_extract_no_obj[..., 0].float(), torch.zeros_like(y_pred_extract_no_obj[..., 0]).float())\n",
    "    \n",
    "    #-------------------------------------------------------- for object class loss\n",
    "    \n",
    "    y_pred_extract_class = y_pred[target_indices][:, 5:]\n",
    "    class_extract = y_true[target_indices][:, 5:]\n",
    "    \n",
    "    class_loss = difference(y_pred_extract_class.float(), class_extract.float())\n",
    "    \n",
    "    #--------------------------------------------------------- for object center loss\n",
    "    \n",
    "    y_pred_extract_center = y_pred[target_indices][:, 1:3]\n",
    "    center_target = y_true[target_indices][:, 1:3]\n",
    "    \n",
    "    center_loss = difference(y_pred_extract_center.float(), center_target.float())\n",
    "    \n",
    "    #------------------------------------------------------- for width and height\n",
    "    \n",
    "    size_pred = y_pred[target_indices][:, 3:5]\n",
    "    size_target = y_true[target_indices][:, 3:5]\n",
    "    \n",
    "    size_loss = difference(torch.sqrt(torch.abs(size_pred.float())), torch.sqrt(torch.abs(size_target.float())))\n",
    "    \n",
    "    box_loss = center_loss + size_loss\n",
    "    \n",
    "    lambda_coord = 5\n",
    "    lambda_no_obj = 0.5\n",
    "    \n",
    "    loss = object_loss + (lambda_no_obj * no_obj_loss) + lambda_coord * box_loss + class_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model and data to MPS device\n",
    "device = torch.device(\"cuda\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "images = images.to(device)\n",
    "boxes = boxes.to(device)\n",
    "\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Assuming batch size of 32 and iterating through batches\n",
    "    batch_size = 32\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        img_batch = images[i:i+batch_size]\n",
    "        target_batch = boxes[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(img_batch)\n",
    "\n",
    "        # Assuming you have a custom YOLO-like loss function\n",
    "        loss = yolo_loss(target_batch,outputs)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print average loss for epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / (len(images) / batch_size)}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in [60,10,5,4,3416,500,600,800,560,1000,595,1400,1657]:\n",
    "    \n",
    "    output = model(images[j:j+32])[:1] #SINCE MODEL TAKES 32 AS BATCH AND NOT SINGLE IMAGE\n",
    "\n",
    "    # Threshold for detection\n",
    "    THRESH = 0.25\n",
    "\n",
    "    # Get object positions\n",
    "    object_positions = torch.where(output[..., 0] >= THRESH)\n",
    "    selected_output = output[object_positions]\n",
    "\n",
    "    # Initialize lists for final boxes and scores\n",
    "    final_boxes = []\n",
    "    final_scores = []\n",
    "\n",
    "    # Iterate through object positions\n",
    "    for i in range(len(object_positions[0])):\n",
    "        if selected_output[i][0] > THRESH:\n",
    "            # Extract box parameters\n",
    "            output_box = selected_output[i][1:5].float()\n",
    "            x_centre = (object_positions[1][i] + output_box[0]) * 32\n",
    "            y_centre = (object_positions[2][i] + output_box[1]) * 32\n",
    "            x_width, y_height = torch.abs(224 * output_box[2]), torch.abs(224 * output_box[3])\n",
    "            x_min, y_min = int(x_centre - (x_width / 2)), int(y_centre - (y_height / 2))\n",
    "            x_max, y_max = int(x_centre + (x_width / 2)), int(y_centre + (y_height / 2))\n",
    "\n",
    "            # Adjust bounding box coordinates\n",
    "            x_min = max(0, x_min)\n",
    "            y_min = max(0, y_min)\n",
    "            x_max = min(224, x_max)\n",
    "            y_max = min(224, y_max)\n",
    "\n",
    "            # Append to final boxes\n",
    "            final_boxes.append([x_min, y_min, x_max, y_max, \n",
    "                               classes[torch.argmax(selected_output[i][5:]).item()]])\n",
    "            final_scores.append(selected_output[i][0].item())\n",
    "\n",
    "\n",
    "    img = np.array(images[j].permute(1, 2, 0).to('cpu')).copy()\n",
    "    \n",
    "    for i in range(len(final_boxes)):\n",
    "\n",
    "        # Extract object classes and boxes\n",
    "        object_classes = final_boxes[i][4]\n",
    "        nms_output = final_boxes[i][0:4]\n",
    "\n",
    "        x1, y1 = nms_output[:2]\n",
    "        x2, y2 = nms_output[2:]\n",
    "\n",
    "        # Specify the color (in BGR format) and thickness\n",
    "        color = (255, 0, 0) \n",
    "        thickness = 2\n",
    "\n",
    "        # Draw the rectangle\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)\n",
    "        \n",
    "        # Add text (object classes) \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.5\n",
    "        font_color = (255, 255, 255)  \n",
    "        text_position = (x1, y2)  \n",
    "\n",
    "        cv2.putText(img, object_classes, text_position, font, font_scale, font_color, thickness)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
